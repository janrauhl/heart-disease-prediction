{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Problem Statement and Objective \nHeart disease describes a range of conditions that affects a person's heart functioning performance. Heart disease describes a range of conditions that affects a person's heart. Heart disease is the leading cause of death in the United States, with about 655,000 Americans die from heart disease each year, and that is about 1 in every 4 deaths (CDC, n.d.). Heart disease includes Arrythymia, Atherosclerosis, Cardioyopathy, Congenital heart defects, Coronary artery disease (CAD), and heart infections, which is caused by various factors, including heart defects, anxiety, diabetes, excessive use of alcohol and caffeine, and others (Whitworth, 2020). As each type of heart diseases has different kind of symptoms and all symptoms are complicated, thus, it is difficult to identify the presence of heart disease. Hence, prediction of heart disease is one of the most important focus in the section of clinical data analysis (Rawat, 2019).\n\nPrediction of heart disease can be achieved by implementing machine learning algorithms on data provided by healthcare institutions. Machine learning algorithms such as Naive Bayes Classification, K-Nearest Neighbour, Logistic Regression, Decision Tree Classification and others, which helps to classifies the presence of heart disease with the given symptoms. \n\nIn this assessment, various classification models will be developed and evaluated, to determine the best model for prediction of heart disease in United States citizens. The model with the best performance will then be used identify if a person with the given conditions is suffering from heart disease or not. The model developed will reduce the burden for healthcare instituition as it is able to identify and predict the presence of heart disease efficiently. "},{"metadata":{},"cell_type":"markdown","source":"## Dataset "},{"metadata":{},"cell_type":"markdown","source":"The dataset used is the Heart Disease Data Set from the Cleveland database, created in 1988 by V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D. (UCI Machine Learning Repository, n.d.). As Cleveland is one of the major city in United States, the dataset used is relevant for use in prediction of heart disease in United States citizen. The original dataset consists of 76 attributes, but published experiments refer the dataset using a subset of 14 attributes, with 303 instances. "},{"metadata":{},"cell_type":"markdown","source":"The dataset description are as below (UCI Machine Learning Repository, n.d.): \n1. age: age in years \n2. sex: 1 = male, 0 = female \n3. cp (4 values): chest pain type \n    - Value 1: typical angina \n    - Value 2: atypical angina \n    - Value 3: non-anginal pain \n    - Value 4: asymptomatic \n4. trestbps: resting blood pressure in mm Hg on admission to the hospital \n5. chol: serum cholestoral in mg/dl\n6. fbs: fasting blood sugar > 120 mg/dl (1 = true; 0 = false) \n7. restecg: resting electrocardiographic results (values 0,1,2)\n    - Value 0: normal \n    - Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) \n    - Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria \n8. thalach: maximum heart rate achieved \n9. exang: exercise induced angina (1 = yes; 0 = no) \n10. oldpeak: ST depression induced by exercise relative to rest \n11. slope: the slope of the peak exercise ST segment \n    - Value 1: upsloping \n    - Value 2: flat \n    - Value 3: downsloping \n12. ca: number of major vessels (0-3) colored by flourosopy \n13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n14. target: 1 = presence, 0 = absence \n\nThe dataset consisting 5 quantitative attributes which are age, trestbps, chol, thalach and oldpeak ; 9 qualitative categorical attributes which are sex, cp, fbs, restecg, exang, slope, ca, thal and target. ca is considered as qualitative categorical attributes as it only consists of 4 type of unique values. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading packages and libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n%matplotlib inline","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/heart-disease-dataset/heart.csv') #directory of dataset \ndataset.head()","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n0   52    1   0       125   212    0        1      168      0      1.0      2   \n1   53    1   0       140   203    1        0      155      1      3.1      0   \n2   70    1   0       145   174    0        1      125      1      2.6      0   \n3   61    1   0       148   203    0        1      161      0      0.0      2   \n4   62    0   0       138   294    1        1      106      0      1.9      1   \n\n   ca  thal  target  \n0   2     3       0  \n1   0     3       0  \n2   0     3       0  \n3   1     3       0  \n4   3     2       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>cp</th>\n      <th>trestbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalach</th>\n      <th>exang</th>\n      <th>oldpeak</th>\n      <th>slope</th>\n      <th>ca</th>\n      <th>thal</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>52</td>\n      <td>1</td>\n      <td>0</td>\n      <td>125</td>\n      <td>212</td>\n      <td>0</td>\n      <td>1</td>\n      <td>168</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53</td>\n      <td>1</td>\n      <td>0</td>\n      <td>140</td>\n      <td>203</td>\n      <td>1</td>\n      <td>0</td>\n      <td>155</td>\n      <td>1</td>\n      <td>3.1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>70</td>\n      <td>1</td>\n      <td>0</td>\n      <td>145</td>\n      <td>174</td>\n      <td>0</td>\n      <td>1</td>\n      <td>125</td>\n      <td>1</td>\n      <td>2.6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>61</td>\n      <td>1</td>\n      <td>0</td>\n      <td>148</td>\n      <td>203</td>\n      <td>0</td>\n      <td>1</td>\n      <td>161</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>62</td>\n      <td>0</td>\n      <td>0</td>\n      <td>138</td>\n      <td>294</td>\n      <td>1</td>\n      <td>1</td>\n      <td>106</td>\n      <td>0</td>\n      <td>1.9</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.shape ","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"(1025, 14)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"The dataset is loaded from the downloaded directory, and the first five rows of dataset is shown above. "},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing \nData preprocessing is the one of the major step in building a machine learning algorithms, as only quality and clean dataset produces quality model that gives quality results. Data preprocessing transforms raw data to useful and efficient format for human and machine learning process. Data preprocessing involves data cleaning, data transformation and data reduction. \n\nData cleaning includes handling missing values and noises, data transformation includes data discretization, concept hierarchy, normalization and standardization. Data reduction involves numerosity reduction and dimensionality reduction (GeeksforGeeks, 2019). "},{"metadata":{},"cell_type":"markdown","source":"## Handling Duplicates and Outliers (Data Cleaning)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#detect missing values\ndataset.isnull().sum()","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"age         0\nsex         0\ncp          0\ntrestbps    0\nchol        0\nfbs         0\nrestecg     0\nthalach     0\nexang       0\noldpeak     0\nslope       0\nca          0\nthal        0\ntarget      0\ndtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Missing values are detected using the `isnull()` syntax. The dataset does not consist any missing values, as shown in the code segment. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.boxplot(data=dataset,x=dataset[\"age\"])","execution_count":15,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-52b48481b816>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"age\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/seaborn/_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             )\n\u001b[1;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mboxplot\u001b[0;34m(x, y, hue, data, order, hue_order, orient, color, palette, saturation, width, dodge, fliersize, linewidth, whis, ax, **kwargs)\u001b[0m\n\u001b[1;32m   2240\u001b[0m     plotter = _BoxPlotter(x, y, hue, data, order, hue_order,\n\u001b[1;32m   2241\u001b[0m                           \u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2242\u001b[0;31m                           width, dodge, fliersize, linewidth)\n\u001b[0m\u001b[1;32m   2243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0max\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, hue, data, order, hue_order, orient, color, palette, saturation, width, dodge, fliersize, linewidth)\u001b[0m\n\u001b[1;32m    404\u001b[0m                  width, dodge, fliersize, linewidth):\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestablish_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestablish_colors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mestablish_variables\u001b[0;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# Figure out the plotting orientation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             orient = infer_orient(\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_numeric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             )\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/seaborn/_core.py\u001b[0m in \u001b[0;36minfer_orient\u001b[0;34m(x, y, orient, require_numeric)\u001b[0m\n\u001b[1;32m   1302\u001b[0m     \"\"\"\n\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m     \u001b[0mx_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mvariable_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mvariable_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/seaborn/_core.py\u001b[0m in \u001b[0;36mvariable_type\u001b[0;34m(vector, boolean_type)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;31m# Special-case all-na data, which is always \"numeric\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"numeric\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1438\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m         raise ValueError(\n\u001b[0;32m-> 1440\u001b[0;31m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1441\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m         )\n","\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data=dataset,x=dataset[\"trestbps\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data=dataset,x=dataset[\"chol\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data=dataset,x=dataset[\"thalach\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data=dataset,x=dataset[\"oldpeak\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The boxplot for quantitative attributes which are age, trestbps, chol, thalach and oldpeak, is plotted and outliers is detected for the mentioned attributes except age, using IQR outlier detection method. The IQR outlier detection method accepts range of values from (Q1 - 1.5 * IQR) to (Q3 + 1.5 * IQR), where Q1 and Q3 represents the first and third quartile respectively and IQR represents the interquartile range. \n\nThe outliers are handled by capping them with the acceptable range of minimum and maximum value, with the Winsorization method. "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print min and max of acceptable range \nmaxtrest = 140 + (1.5)*(140-120) \nprint(\"Max of accpetable range of trestbps: \",maxtrest)\nmintrest = 120 - (1.5)*(140-120)\nprint(\"Min of acceptable range of trestbps: \",mintrest) \nmaxchol = 275 + (1.5)*(275-211) \nprint(\"Max of accpetable range of chol: \",maxchol)\nminchol = 211 - (1.5)*(275-211)\nprint(\"Min of acceptable range of chol: \",minchol) \nmaxthal = 166 + (1.5)*(166-133) \nprint(\"Max of accpetable range of thalach: \",maxthal)\nminthal = 133 - (1.5)*(166-133)\nprint(\"Min of acceptable range of thalach: \",minthal) \nmaxpeak = 1.6+ (1.5)*(1.6-0) \nprint(\"Max of accpetable range of oldpeak: \",maxpeak)\nminpeak = 0 - (1.5)*(1.6-0)\nprint(\"Min of acceptable range of oldpeak: \",minpeak) \n\n#since thalach is integer attribute  \nminthal = 84 #rounded to integer \nmaxthal = 216 #rounded to integer ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cap outliers with min and max of acceptable range \nfor i in range(len(dataset[\"trestbps\"])):\n    if dataset[\"trestbps\"][i] > maxtrest:\n        dataset[\"trestbps\"][i]= maxtrest \n    if dataset[\"trestbps\"][i] < mintrest:\n        dataset[\"trestbps\"][i]= mintrest\nfor i in range(len(dataset[\"chol\"])):\n    if dataset[\"chol\"][i] > maxchol:\n        dataset[\"chol\"][i]= maxchol \n    if dataset[\"chol\"][i] < minchol:\n        dataset[\"chol\"][i]= minchol\nfor i in range(len(dataset[\"thalach\"])):\n    if dataset[\"thalach\"][i] > maxthal:\n        dataset[\"thalach\"][i]= maxthal \n    if dataset[\"thalach\"][i] < minthal:\n        dataset[\"thalach\"][i]= minthal \nfor i in range(len(dataset[\"oldpeak\"])):\n    if dataset[\"oldpeak\"][i] > maxpeak:\n        dataset[\"oldpeak\"][i]= maxpeak \n    if dataset[\"oldpeak\"][i] < minpeak:\n        dataset[\"oldpeak\"][i]= minpeak \ndataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.boxplot() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is now free from duplicates and outliers. \n"},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection (Data Reduction) \nFeature selection is done by using the filter method, as various machine learning algorithms will be applied to build various models. Filter method is chosen as wrapper method such as Reduce Feature Elimination (RFE) function in scikit-learn is limited to certain machine learning algorithms such as linear regression and decision tree. \n\nIn filter method, only relevant features are taken. Features with absolute correlation coefficient of greater than 0.5 with the target and absolute correlation coefficient of less than 0.5 with other predictors is chosen. This is to reduce overfitting and multicollinearity, which will cause misleading prediction. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation matrix \nplt.figure(figsize=(12,10))\ncor = dataset.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.drop('target', axis=1).corrwith(dataset.target).plot(kind = 'bar', grid = True, \n                                                             figsize = (12, 8), \n                                                             title = \"Correlation with Target\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As all attributes has a absolute correlation coefficient of less than 0.5 with 'target', thus, a bar-form correlation of features with 'target' is plotted to have better insights of the correlation. From the bar-form correlation matrix, chol and fbs has the least correlation with the target. However, all predictors or independent variable has a absolute correlation coefficient of less than 0.5 with each other and also with 'target', thus, no attributes are removed from the dataset. "},{"metadata":{},"cell_type":"markdown","source":"## Dummy Variables (Data Transformation) \nData transformation is done by converting categorical attributes to dummy variables for the ease of model construction, using the`.get_dummies()` syntax. Dummy variables uses 0 or 1 to indicate absence or presence of the categorical value, sorting them to mutually exclusive categories. Since dummy varialbes is created, hence, there is no need to perform concept hierarchy or relevelling to reduce the factor levels of the attributes. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#categorical variables to convert to dummy variables \nfor column in dataset.columns:\n    if len(dataset[column].unique()) <= 10:\n        print(f\"{column} : {dataset[column].unique()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert to dummy variables \nfrom pandas import get_dummies\n\na = pd.get_dummies(dataset['sex'], prefix = \"sex\")\nb = pd.get_dummies(dataset['cp'], prefix = \"cp\")\nc = pd.get_dummies(dataset['fbs'], prefix = \"fbs\")\nd = pd.get_dummies(dataset['restecg'], prefix = \"restecg\")\ne = pd.get_dummies(dataset['exang'], prefix = \"exang\")\nf = pd.get_dummies(dataset['slope'], prefix = \"slope\")\ng = pd.get_dummies(dataset['ca'], prefix = \"ca\")\nh = pd.get_dummies(dataset['thal'], prefix = \"thal\")\n\n#data frame with dummy variables \nframes = [dataset, a, b, c, d, e, f, g, h]\n\n#combine dummy variables with dataset \ndataset2 = pd.concat(frames, axis = 1)\n\n#drop categorical variabes as they are converted to dummy variables\ndataset2 = dataset2.drop(columns = ['sex','cp', 'fbs', 'restecg',\n                                  'exang','slope','ca','thal'])\n\ndataset2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA) \nExploratory Data Analysis (EDA) analyses the dataset, gives insight about dataset with visuals, to discover patterns and abnormalies. \n\nPart of the EDA has already performed in data preprocessing for outlier detection using Quartile method with boxplot, and for data reduction using correlation matrix plot. \n\nEDA process involves exploring the dimension of dataset, central tendency and dispersion of dataset, summary statistics and others, which will be demonstrated below. "},{"metadata":{},"cell_type":"markdown","source":"## Dimension and Variables or Attributes of Dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"type(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is a dataframe with dimension consists of 31 features including the dummy variables, and 301 instances, as shown using the `.shape` syntax."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset2.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset originally consists of 14 attributes where 5 of them are quantitative attributes including integers and float, and 9 of them are qualitative categorical attributes which were encoded to integers in the raw dataset. These categorical attributes are then transformed to dummy variables in data transformation, resulting in 31 columns in total. "},{"metadata":{},"cell_type":"markdown","source":"## Frequency Distribution Table \nFrequency Distribution Table for Categorical Variables are shown below, which measures if the dataset is biased or having imbalance class or not. "},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(index=dataset['sex'], columns='count') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(index=dataset['cp'], columns='count') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(index=dataset['fbs'], columns='count') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(index=dataset['restecg'], columns='count') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(index=dataset['exang'], columns='count') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(index=dataset['slope'], columns='count') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(index=dataset['ca'], columns='count') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(index=dataset['thal'], columns='count') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(index=dataset['target'], columns='count') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"target\", data=dataset, palette=\"bwr\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As shown in the plot, the distribution of 'heart disease' and 'no heart disease' is considered evenly distribute as there are no big difference in count. Hence, this dataset will produce a good model as it is not biased.  "},{"metadata":{},"cell_type":"markdown","source":"## Measure of Central Tendency and Dispersion "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.mode()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Measures of central tendency of the dataset measures the center or middle of the dataset, which can be measured using the mean, median and mode (CK-12,2020). \n\nAs shown in `.describe()` and `.mode()`, the mean, median (2nd Quartile or 50%) and mode for the quantitative attributes are approximately the same, thus they are approximately normal. This can be seen from the boxplot plotted in data cleaning too, as the boxes are approximately evenly separated. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(3, 2, figsize=(30,50))\n\n#scatter plot of radius and compactness\naxes[0,0].scatter(dataset['age'], dataset['target'])\naxes[0,0].set_title(\"Age Vs. Target\")\naxes[0,0].set_xlabel(\"Age\")\naxes[0,0].set_ylabel(\"Target\")\n\n#scatter plot of radius and texture\naxes[0,1].scatter(dataset['trestbps'], dataset['target'])\naxes[0,1].set_title(\"Trestbps Vs. Target\")\naxes[0,1].set_xlabel(\"Trestbps\")\naxes[0,1].set_ylabel(\"Target\")\n\n#scatter plot of radius and smoothness\naxes[1,0].scatter(dataset['chol'], dataset['target'])\naxes[1,0].set_title(\"Chol Vs. Target\")\naxes[1,0].set_xlabel(\"Chol\")\naxes[1,0].set_ylabel(\"Target\")\n\n#scatter plot of radius and concavity \naxes[1,1].scatter(dataset['thalach'], dataset['target']);\naxes[1,1].set_title(\"Thalach Vs. Target\");\naxes[1,1].set_xlabel(\"Thalach\")\naxes[1,1].set_ylabel(\"Target\")\n\naxes[2,0].scatter(dataset['oldpeak'], dataset['target']);\naxes[2,0].set_title(\"Oldpeak Vs. Target\");\naxes[2,0].set_xlabel(\"Oldpeak\")\naxes[2,0].set_ylabel(\"Target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(3, 2, figsize=(30,50))\n\n#scatter plot of radius and compactness\naxes[0,0].hist(dataset['age'])\naxes[0,0].set_title(\"Histogram of Age\")\naxes[0,0].set_xlabel(\"Age\")\naxes[0,0].set_xlim((min(dataset.age), max(dataset.age)))\n\n#scatter plot of radius and texture\naxes[0,1].hist(dataset['trestbps'])\naxes[0,1].set_title(\"Histogram of Trestbps\")\naxes[0,1].set_xlabel(\"Trestbps\")\naxes[0,1].set_xlim((min(dataset.trestbps), max(dataset.trestbps)))\n\n#scatter plot of radius and smoothness\naxes[1,0].hist(dataset['chol'])\naxes[1,0].set_title(\"CHistogram of Chol\")\naxes[1,0].set_xlabel(\"Chol\")\naxes[1,0].set_xlim((min(dataset.chol), max(dataset.chol)))\n\n#scatter plot of radius and concavity \naxes[1,1].hist(dataset['thalach']);\naxes[1,1].set_title(\"Histogram of Thalach\");\naxes[1,1].set_xlabel(\"Thalach\")\naxes[1,1].set_xlim((min(dataset.thalach), max(dataset.thalach)))\n\naxes[2,0].hist(dataset['oldpeak']);\naxes[2,0].set_title(\"Histogram of Oldpeak\");\naxes[2,0].set_xlabel(\"Oldpeak\")\naxes[2,0].set_xlim((min(dataset.oldpeak), max(dataset.oldpeak)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = dataset.std()/dataset.mean()\ncv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Measures of dispersion of the dataset measures how disprese or spread out the data in the dataset are. It can be measured using range, standard deviation and variance, and also coefficient of variation (Kaufmann, 2014). \n\nThe histogram and scatter plot for quantitative variales is also able to show the dispersion and central tendancy of the data points. As shown in the histograms and scatter plots, all the attributes are considered less disperse.\n\nThe Coefficient of Variation (CV) is calculated by dividing standard deviation by mean. A CV value of lesser than 1 indicates a low variance. However, oldpeak has CV value of 1.077, thus it is considered as high variance, thus oldpeak is spread out (Kaufmann, 2014). "},{"metadata":{},"cell_type":"markdown","source":"## Skewness of Dataset \nSkewness measures a dataset’s symmetry – or lack of symmetry.   A perfectly symmetrical data set will have a skewness of 0.   The normal distribution has a skewness of 0 (McNeese, 2016). "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accoding to rule of thumb, skewness of -0.5 to 0.5 is fairly symmetrical. Skewness with absolute value of 0.5 to 1 is moderately skewed, and highly skewed if more than 1. The mathematical symbol in skewness value indicates whether the variables is postiviely or negatively skewed. \n\nage, sex, thalach, slope, thal and target is negatively skewed, and other attributes are positively skewed. \n\nFrom `.skew()`, fbs and ca are highly skewed. sex, cp, exang, oldpeak are moderately skewed. "},{"metadata":{},"cell_type":"markdown","source":"## Correlation Matrix \nCorrelation Matrix heatmap is shown in Feature Selection or Data Reduction in Data Preprocessing. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation matrix\ndataset.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As shown in the correlation matrix, all attributes has absolute correlation of less than 0.5 with each other, hence, there will be no multicollinearity. "},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.pairplot(dataset, diag_kind=\"kde\")\ng.map_lower(sns.kdeplot, levels=4, color=\".2\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scatter plot matrix is a matrix of scatter plots where each scatter plot in the grid is created between different combinations of variables. In other words, scatter plot matrix represents bi-variate or pairwise relationship between different combinations of variables in grid form (Kumar, 2020). Diagonally from top left to right, the plots represent univariate distribution of data for the variable in that column, for instance, distribution of age, distribution of sex, and others. Scatter plot matrix is useful in investigating the feature correlation and multicollinearity. \n\nThe above scatter density plot matrix is associated with Kernel Density Estimation (KDE) plots, which estimates the probability density of a variable and also the probability distribution (JournalDev, n.d.). The higher density of KDE plots implies the high probability of occurence. \n\nFrom the scatter density plot matrix, all variables are moderately related to target. However, all the scatter plot are dispersed with correlation of absolute value less than 0.5, hence, no multicollinearity. "},{"metadata":{},"cell_type":"markdown","source":"## Visualization of Attributes \nAttributes are visualized to discover general patterns in the attributes, and have a rough conclusion about the attributes. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x=dataset.chol[dataset.target==1], y=dataset.thalach[(dataset.target==1)], c=\"red\")\nplt.scatter(x=dataset.chol[dataset.target==0], y=dataset.thalach[(dataset.target==0)])\nplt.legend([\"Disease\", \"Not Disease\"])\nplt.xlabel(\"Serum cholestoral (mg/dl)\")\nplt.ylabel(\"Maximum Heart Rate\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the scatter plot, a higher heart rate implies a higher chance of having heart disease."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x=dataset.trestbps[(dataset.target==1)], y=dataset.thalach[dataset.target==1], c=\"red\")\nplt.scatter(x=dataset.trestbps[(dataset.target==0)], y=dataset.thalach[dataset.target==0])\nplt.legend([\"Disease\", \"Not Disease\"])\nplt.xlabel(\"Resting blood pressure on admission (mmHg)\")\nplt.ylabel(\"Maximum Heart Rate\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the scatter plot, person with high heart rate and low blood pressure will have higher chance to suffer from heart disease. "},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(dataset.sex,dataset.target).plot(kind=\"bar\",figsize=(15,6),color=[\"blue\",\"red\"])\nplt.title('Heart Disease Frequency for Sex')\nplt.xlabel('Sex')\nplt.xticks(rotation = 0)\nplt.ylabel('Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the bar chart, female tends to have higher risk to suffer from heart disease. "},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(dataset.age,dataset.target).plot(kind=\"bar\",figsize=(20,10))\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the bar chart of Age versus frequency, age of 41 to 45 and 51 to 54 are having a high amount of heart disease, impling that they are highly risked people to have heart disease. \n\nHowever, the above conclusions are induced from the basic visualization of the attributes. It may not be true since it is comparing in a univariate or bivariate way. A precise conclusion or rules should be extracted from the classification model. "},{"metadata":{},"cell_type":"markdown","source":"## Realibility - Fitting Probability Distribution "},{"metadata":{},"cell_type":"markdown","source":"Probability distribution is fitted to the quantitative variables, to select the best probability model. Statistical approaches to estimating how well a given model fits a dataset and how complex the model is, calculating using the Akaike Information Criterion (AIC) which is derived from frequentist probability and Bayesian Information Criterion (BIC) which is derived from Bayesian probability. AIC and BIC are both calculated using the maximum likelihood, which maximizes the conditional probability of observing the data (X) given a specific probability distribution and its parameters (theta) (Brownlee, 2019). \n\nAIC and BIC works in a similar way but they focus on different situation. AIC attempt to select an unknown model which has high dimensional reality whereas BIC finds only True models. In layman terms, AIC indicates the overfitting issues and BIC indicates the underfitting issues, as they both penalize free parameters (Ash3323 and Taylor, 2018).\n\nThus, in this assessment, AIC and BIC are both taken into consideration in choosing the best probability distribution model. The lower score of AIC and BIC indicates a better probability distribution model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from reliability.Fitters import Fit_Everything\nFit_Everything(failures=np.array(dataset['age']), show_histogram_plot=True, show_probability_plot=True, show_PP_plot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from reliability.Fitters import Fit_Everything\nFit_Everything(failures=np.array(dataset['trestbps']), show_histogram_plot=True, show_probability_plot=True, show_PP_plot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from reliability.Fitters import Fit_Everything\nFit_Everything(failures=np.array(dataset['chol']), show_histogram_plot=True, show_probability_plot=True, show_PP_plot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from reliability.Fitters import Fit_Everything\nFit_Everything(failures=np.array(dataset['thalach']), show_histogram_plot=True, show_probability_plot=True, show_PP_plot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from reliability.Fitters import Fit_Everything\nFit_Everything(failures=np.array(dataset['oldpeak']), show_histogram_plot=True, show_probability_plot=True, show_PP_plot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Weibull 2P best fits for the all quantitative attributes, as Weibull 2P has low AIC and BIC compared to other distribution fitted for the quantitative attributes. \n\nThe Weibull distribution is widely used in reliability and life data analysis due to its versatility. Life data analysis refers to data analysis involving prediction of lifetime. The Weibull 2p or 2 parameters distribution consists parameters of shape and scale, where Weibull 3p or 3 parameters distribution consists paramters of shape, scale and location (Weibull, n.d.). Weibull distribution is commonly used assess product reliability, analyze life data and model failure times, from biology industry to economic industry (Stephanie, 2017). "},{"metadata":{},"cell_type":"markdown","source":"# Model Development \n`dataset2` is used for model development which consists of dummy variables created from categorical variables, as shown in Data Preprocessing while doing Data Transformation.\n\nDecision Tree Classifier, Naive Bayes Classifier and K-Nearest Neighbours will be modelled and evaluated, to choose the best model with high performance metrics. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset2.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.tree import plot_tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n#split dataset to train and test \nfeature = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'sex_0', 'sex_1', 'cp_0', 'cp_1', 'cp_2', 'cp_3', 'fbs_0', 'fbs_1', 'restecg_0', 'restecg_1', 'restecg_2', 'exang_0', 'exang_1',\n       'slope_0', 'slope_1', 'slope_2', 'ca_0', 'ca_1', 'ca_2', 'ca_3', 'ca_4', 'thal_0', 'thal_1', 'thal_2', 'thal_3'],\n\n#split train and test for decision tree model 80/20 split\ny = dataset2[\"target\"]\nx = dataset2.drop([\"target\"],axis=1).values\nxtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size = 0.2, random_state=0)\n\nxtrainknn = scaler.fit_transform(xtrain)\nxtestknn = scaler.transform(xtest)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrainknn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtestknn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtest ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K-Nearest Neighbours (KNN) Modelling \nK-Nearest Neighbours (KNN) is a supervised learning algorithm which is also a classification algorithm. It considers K number of neighbours nearest the unknown and newly added point, which classifies the unknown point based on the neighbours' votes, for instance, if 5 out of 6 neighbours has a target of presence of heart disease, then the newly added unknown point will be classified as presence of heart disease. The distance of the newly added point with the neighbours can be calculated using Euclidean distance formula, and the distance is then sorted to find the nearest neighbours.\n\nSince K-Nearest is dependent on the major votes of the neighbours, it is easily affected by attributes with large range as these attributes tends to dominate the other attributes which in turn affects the model performance. Hence, normalization is done to have a good KNN model performance. Normalization is done after Train and Test dataset split using the MinMaxScaler() (Kedarps, 2017). "},{"metadata":{"trusted":true},"cell_type":"code","source":"#suggested number of neighbours \nimport math  \nmath.sqrt(301)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to determine optimum number of neighbours\nscoreList = []\nfor i in range(1,21):\n    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n    knn2.fit(xtrainknn, ytrain)\n    scoreList.append(knn2.score(xtestknn, ytest))\n    \nplt.plot(range(1,21), scoreList)\nplt.xticks(np.arange(1,21,1))\nplt.xlabel(\"K value\")\nplt.ylabel(\"Score\")\nplt.show()\n\nacc = max(scoreList)*100\nprint(\"Maximum KNN Score is {:.2f}%\".format(acc))\nprint(\"K Value with highest score: \", (scoreList.index(max(scoreList))+1)) #+1 as index starts from 0 in array","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to research, there are no optimal K number of neighbours which suits all kind of dataset, as each dataset has different requirements. It is suggested to have `sqrt(number of instances)` of neighbours, however, this is just as a guideline to decided the number of neighbours (Navlani, 2018). \n\nHence, to determine the optimal K number of neighbours for this case study, the model performance scorse is calculated and assessed for K of 1 to 20 using looping. From the plot shown above, the optimal K value is 10 with a model performance score of 88.52%. Thus, K value for this case study is set to 10. Whenver a new unknown record is added, it will be classified based on its 10 nearest neighbours' votes. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#modeling KNN \nknn = KNeighborsClassifier(n_neighbors = 10)\nknn.fit(xtrainknn, ytrain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Classification \nDecision Tree Classification is a supervised machine learning algorithm, which has a tree like structure, consisting of nodes, leaves, and branches. Data are partitioned recursively from the root node to leaf node. As decision tree mimic the way human makes decision, thus, it is easy to understand and interpret by humans. Decision trees has the capability of handling high dimensionality data with good accuracy. \n\nThere are various algorithms to build decision tree such as ID3, CART, C4.5 and others. The `DecisionTreeClassifier()` from Sckit-learn is using CART algorithm to build decision tree. However, Decision Tree is easily affected by imbalanced or biased dataset. Since the dataset used in this case study is not biased, hence, it is suitable to use Decision Tree for modeling (Navlani, 2018). \n\nDecision trees are not affected by monotonic transformation such as normalization, hence, normalization is not needed (Rapaio, 2015). "},{"metadata":{"trusted":true},"cell_type":"code","source":"#to determine optimum number of maximum leaf nodes \nscoreList = []\nfor i in range(2,21):\n    dt2 = DecisionTreeClassifier(max_leaf_nodes = i)  # n_neighbors means k\n    dt2.fit(xtrain, ytrain)\n    scoreList.append(dt2.score(xtest, ytest))\n    \nplt.plot(range(2,21), scoreList)\nplt.xticks(np.arange(2,21,1))\nplt.xlabel(\"Max Leaf Node\")\nplt.ylabel(\"Score\")\nplt.show()\n\nacc = max(scoreList)*100\nprint(\"Maximum Max Leaf Node Score is {:.2f}%\".format(acc))\nprint(\"Max Leaf Node with highest score: \", (scoreList.index(max(scoreList))+1)) #+1 as index starts from 0 in array","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The optimal number of maximum leaf nodes is calculated using the loop, to determine the best model score with maximum leaf node ranged from 2 to 20. From the plot above, the optimal maximum number of leaf nodes is 6 to 9 with model score of 77.05%. Hence, the maximum leaf node is set to 9 to provide a better accuracy results. "},{"metadata":{"trusted":true},"cell_type":"code","source":"DT = DecisionTreeClassifier(max_leaf_nodes=9, random_state=0)\n#since max leaf node of 6 to 9 has the same scores, thus, 9 is used for better accuracy. \nDT = DT.fit(xtrain,ytrain)\nplt.figure(figsize=(50,30))\na = plot_tree(DT, \n              feature_names=list(dataset2.drop([\"target\"], axis=1)), \n              class_names=[\"Heart Disease\",\"No Heart Disease\"], \n              filled=True, \n              rounded=True, \n              fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes Classifier \nNaive Bayes Classifier, the most straightforward and fast classification algorithm, suitable for use in high dimensionality dataset. Naive Bayes Classifier is a collection of classification algorithm based on Bayes Theorem which take likelihood, prior probability and posterior probability of an event into account. \n\nNaive Bayes Classifier assumes that each predictors are independent from each other, which implies that it requires minimal multicollinearity. As the dataset used in this case study has little to no multicollinearity, hence, Naive Bayes Classifier is suitable for this case study. \n\nNaive Bayes Classifier also assumes that each predictors contributes equally to the outcome, which implies that they all have the same probability to contribute to the outcome (Khurana, 2020). \n\nLike Decision Tree Classifier, Naive Bayes Classifier is not affected by monotonic transformation, hence, normalization is not required (Dernoncourt, 2016). "},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = GaussianNB()\nnb.fit(xtrain,ytrain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performance Evaluation of K-Nearest Neighbour, Decion Tree and Naive Bayes Classification Model \nModel performance of KNN, Decition Tree and Naive Bayes Classifier is calculated and evaluated using accuracy, F1 score and confusion matrix. This is to determine the best model that best fit this case study of prediction of presence of heart disease. \n\nThere are many ways to evalute the model performance, depending on the model. For classification model, confusion matrix is used; for regression model, methods such as root mean square error (RMSE), mean absolute error (MAE), relative absolute error (RAE) and others are used (Singh, 2019). \n\nSince the model built in this case study is classification model, thus, confusion matrix is used. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#validate model by using test set to predict \ndt_pred = DT.predict(xtest)\nknn_pred = knn.predict(xtestknn)\nnb_pred = nb.predict(xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n#knn\nprint(\"Accuracy of KNN: \", accuracy_score(ytest, knn_pred) )\nprint(\"F1 Score of KNN: \", f1_score(ytest, knn_pred) )\nknncm = confusion_matrix(ytest, knn_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(knncm, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#decision tree\nprint(\"Accuracy of Decision Tree: \", accuracy_score(ytest, dt_pred) )\nprint(\"F1 Score of Decision Tree: \", f1_score(ytest, dt_pred) )\ndtcm = confusion_matrix(ytest, dt_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(dtcm, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#nb\nprint(\"Accuracy of Naive Bayes: \", accuracy_score(ytest, nb_pred) )\nprint(\"F1 Score of Naive Bayes: \", f1_score(ytest, nb_pred) )\nnbcm = confusion_matrix(ytest, nb_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(nbcm, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy measures how correct the predictions are, precision measures how often the model is correct when predicting the positive class, and recall measures how many actual positives are predicted correctly (Algorithmia, 2020). As accuracy sometimes may be misleading, thus, F1 score is also used to evaluate model performance. F1 score is a harmonic balance between precision and recall, which is commonly used for biased dataset.  \n\nIn this case study, the worst case scenario is the False Negatives (FN) which is located in the top right of the confusion matrix. In FN, patients with heart disease is classified as no heart disease, this might cause delay in treatment for the patient due to misdiagnosis and wrong prediction by the model, causing death in serious cases. From the confusion matrix, KNN model has the lowest FN among the other models, hence, KNN is more preferable. \n\nThe accuracy and F1 score of KNN is also higher compared with Decision Tree and Naive Bayes Classifier, with accuracy of 88.5% and F1 score of 87.7%. \n\nThus, KNN is chosen to be the model to predict presence of heart disease in this case study. "},{"metadata":{},"cell_type":"markdown","source":"## Interpretation of Model  \nThe KNN model is interpreted by giving an unknown new data for prediction as shown below."},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = [[55,140,250,160,2.5, \n        0, 1, \n        0,0,0,1,\n        0,1,\n        0,0,1,\n        0,1,\n        0,1,0,\n        1,0,0,0,0,\n        1,0,0,0]]\n\nDF = pd.DataFrame(data,columns = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', \n                                  'sex_0', 'sex_1', \n                                  'cp_0', 'cp_1', 'cp_2', 'cp_3', \n                                  'fbs_0', 'fbs_1', \n                                  'restecg_0', 'restecg_1', 'restecg_2', \n                                  'exang_0', 'exang_1',\n                                  'slope_0', 'slope_1', 'slope_2', \n                                  'ca_0', 'ca_1', 'ca_2', 'ca_3', 'ca_4', \n                                  'thal_0', 'thal_1', 'thal_2', 'thal_3'])\npred1 = knn.predict(data)\npred1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the KNN model prediction, a male which is 55-years-old with trestbps of 140, chol of 250, thalach of 160, oldpeak of 2.5, with asymptomatic chest pain, with fasting blood sugar of greater than 120mg/dl, resting electrocardiographic showing proabable or definite left ventricular hypertropy by Estes' criteria, with exercise induced angina, with upsloping slope of the peak exercise ST segment, 0 number of major vessels colored by flourosopy and normal thal is predicted to suffer from heart disease. "},{"metadata":{},"cell_type":"markdown","source":"The KNN model devolped for this case study is considered performing well with accuracy of 88.5%. This high accuracy model can be used as reference for healthcare instituitions for prediction of presence of heart disease in United States citizens. \n\nHowever, the KNN model will perform better if it was trained with a larger dataset with more instances, it will help improve the KNN model's capability to predict unknown data. \n\nIt is also suggested to train the model with dataset from different locations and countries, such as dataset from Asia. As the KNN model in this case study is built using a United States based dataset, the model tends to overfit, which implies that the model is only capable to predict heart disease of United States citizen, but not citizens from other country. This is because different countries has different culture and lifestyle, and different ethnics has different genetic which also contributes to the causes of heart disease, thus, the KNN model developed in this case study is more biased for United States citizens. \n\nIn conclusion, the KNN model best fits the case study to predict if a United States citizens suffers from heart disease or not. It will not only reduce the cost of diagnosis, but also improves the efficiency in diagnosis and treatment, as patients are diagnosed efficiently and early treatment can be given for better recovery. This would help to reduce the mortality rate caused by heart disease. Suggestions mentioned above can be implemented for a better prediction model that will be used not only in United States but also all over the world with high accuracy and precise prediction. "},{"metadata":{},"cell_type":"markdown","source":"## References \n"},{"metadata":{},"cell_type":"markdown","source":"Algorithmia. 2020. Evaluating machine learning models with a confusion matrix. [online] Available at: <https://algorithmia.com/blog/evaluating-machine-learning-models-with-a-confusion-matrix> [Accessed 4th December 2020] \n\nAsh3323, Taylor, J. 2018. Is there any reason to prefer the AIC or BIC over the other?. [online] Available at: <https://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other> [Accessed 4th December 2020]\n\nBrownlee, J. 2019. Probabilistic Model Selection with AIC, BIC, and MDL. [online] Available at: <https://machinelearningmastery.com/probabilistic-model-selection-measures/> [Accessed 4th December 2020] \n\nCDC. n.d. Heart Disease Facts. [online] Available at: <https://www.cdc.gov/heartdisease/facts.htm> [Accessed: 4th December 2020]\n\nCK-12. 2020. 11.9 Measures of Central Tendency and Dispersion. [online] Available at: <https://www.ck12.org/book/ck-12-basic-algebra-concepts/section/11.9/> [Accessed 4th December 2020] \n\nDernoncourt, F. 2016. With the Naive Bayes classifier, why do we have to normalize the probabilities after calculating the probabilities of each hypothesis?. [online] Available at: <https://stats.stackexchange.com/questions/249762/with-the-naive-bayes-classifier-why-do-we-have-to-normalize-the-probabilities-a> [Accesse 4th December 2020] \n\nGeeksforGeeks. 2019. Data Preprocessing in Data Mining. [online] Available at: <https://www.geeksforgeeks.org/data-preprocessing-in-data-mining/> [Accessed 4th December 2020]\n\nJornalDev. n.d. Seaborn Kdeplot – A Comprehensive Guide. [online] Available at: <https://www.journaldev.com/40204/seaborn-kdeplot> [Accessed 4th December 2020] \n\nKaufmann, J. 2014. What do you consider a good standard deviation?. [online] Available at: <https://www.researchgate.net/post/What-do-you-consider-a-good-standard-deviation> [Accessed 4th December 2020] \n\nKedarps. 2017. Why do you need to scale data in KNN. [online] Available at: <https://stats.stackexchange.com/questions/287425/why-do-you-need-to-scale-data-in-knn> [Accessed 4th December 2020]\n\nKhurana, S. 2020. Naive Bayes Classifiers. [online] Available at: <https://www.geeksforgeeks.org/naive-bayes-classifiers/> [Accessed 4th December 2020] \n\nKumar, A. 2020. What, When, and How of Scatterplot Matrix in Python - Data Analytics. [online] Available at: <https://dzone.com/articles/what-when-amp-how-of-scatterplot-matrix-in-python> [Accessed 4th December 2020] \n\nMcNeese, B. 2016. Are the Skewness and Kurtosis Useful Statistics?. [online] Available at: <https://www.spcforexcel.com/knowledge/basic-statistics/are-skewness-and-kurtosis-useful-statistics> [Accessed 4th December 2020] \n\nNavlani, A. 2018. KNN Classification using Scikit-learn. Available at: <https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn?utm_source=adwords_ppc&utm_campaignid=1455363063&utm_adgroupid=65083631748&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=332602034358&utm_targetid=aud-392016246653:dsa-429603003980&utm_loc_interest_ms=&utm_loc_physical_ms=9066765&gclid=Cj0KCQiA2af-BRDzARIsAIVQUOchHjPNlt2tMCGQWrunwsDqPRMJEiQyZ5UXcATkf8I2QPQjl1Q2IcQaAsvCEALw_wcB> [Accessed 4th December 2020]\n\nNavlani, A. 2018. Decision Tree Classification in Python. Available at: <https://www.datacamp.com/community/tutorials/decision-tree-classification-python> [Accessed 4th December 2020] \n\nRapaio. 2015. Do you have to normalize data when building decision trees using R?. Available at: <https://datascience.stackexchange.com/questions/5277/do-you-have-to-normalize-data-when-building-decision-trees-using-r> [Accessed 4th December 2020] \n\nRawat, S. 2019. Heart Disease Prediction. [online] Available at: <https://towardsdatascience.com/heart-disease-prediction-73468d630cfc> [Accessed 4th December 2020]\n\nSingh, D. 2019. What is Predictive Model Performance Evaluation. [online] Available at: <https://medium.com/@divyacyclitics15/what-is-predictive-model-performance-evaluation-8ef117ae0e40> [Accessed 5th November 2020]\n\nStephanie. 2017. Weibull Distribution and Weibull Analysis. [online] Available at: <https://www.statisticshowto.com/weibull-distribution/> [Accessed 4th December 2020] \n\nUCI Machine Learning Repository. n.d. Heart Disease Data Set. [online] Available at: <https://archive.ics.uci.edu/ml/datasets/heart+disease> [Accessed 4th December 2020] \n\nWeibull. n.d. Characteristics of the Weibull Distribution. [online] Available at: <https://www.weibull.com/hotwire/issue14/relbasics14.htm> [Accessed 4th December 2020] \n\nWhitworth, G. 2020. Everything You Need To Know About Heart Disease. [online] Available at: <https://www.healthline.com/health/heart-disease> [Accessed 4th December 2020] \n\n"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}